{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e140ae-30ab-46eb-9a9a-d9c0f2a359ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# FEATURES = {\n",
    "#     \"af\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\",\"PRSice2\"],\n",
    "#     \"chd\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\",\"LDpred\"],\n",
    "#     \"chf\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\",\"PRSice2\"],\n",
    "#     \"dem\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\",\"LDpred\"],\n",
    "#     \"dia\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\",\"LDpred\"],\n",
    "#     \"stroke\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\",\"Lasso\"]\n",
    "# }\n",
    "\n",
    "# FEATURES = {\n",
    "#     \"af\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\"],\n",
    "#     \"chd\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\"],\n",
    "#     \"chf\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\"],\n",
    "#     \"dem\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\"],\n",
    "#     \"dia\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\"],\n",
    "#     \"stroke\": [\"AGE\",\"SEX\",\"VENT_RT\",\"alcohol\",\"TRIG\",\"BG\",\"CREAT\",\"SBP\",\"DBP\",\"HGT\",\"DLVH\",\"CALC_LDL\",\"HDL\",\"CPD\",\"HIP\"]\n",
    "# }\n",
    "\n",
    "#prs\n",
    "FEATURES = {\n",
    "    \"af\": [\"AGE\",\"SEX\",\"PRSice2\"],\n",
    "    \"chd\": [\"AGE\",\"SEX\",\"LDpred\"],\n",
    "    \"chf\": [\"AGE\",\"SEX\",\"PRSice2\"],\n",
    "    \"dem\": [\"AGE\",\"SEX\",\"LDpred\"],\n",
    "    \"dia\": [\"AGE\",\"SEX\",\"LDpred\"],\n",
    "    \"stroke\": [\"AGE\",\"SEX\",\"Lasso\"]\n",
    "}\n",
    "\n",
    "# FILES = {\n",
    "#     \"dia\":    \"/Data/taegun/prs_revision/data/df_diabet_phenotype_final.csv\",\n",
    "#     \"chf\":    \"/Data/taegun/prs_revision/data/df_chf_phenotype_final.csv\",\n",
    "#     \"chd\":    \"/Data/taegun/prs_revision/data/df_chd_phenotype_final.csv\",\n",
    "#     \"stroke\": \"/Data/taegun/prs_revision/data/df_stroke_phenotype_final.csv\",\n",
    "#     \"af\":     \"/Data/taegun/prs_revision/data/df_af_phenotype_final.csv\",\n",
    "#     \"dem\":    \"/Data/taegun/prs_revision/data/df_dem_phenotype_final.csv\"\n",
    "# }\n",
    "FILES = {\n",
    "    \"dia\":    \"/Data/taegun/prs_revision/data/df_diabet_match_pcr_final2.csv\",\n",
    "    \"chf\":    \"/Data/taegun/prs_revision/data/df_chf_match_pcr_final2.csv\",\n",
    "    \"chd\":    \"/Data/taegun/prs_revision/data/df_chd_match_pcr_final2.csv\",\n",
    "    \"stroke\": \"/Data/taegun/prs_revision/data/df_stroke_match_pcr_final2.csv\",\n",
    "    \"af\":     \"/Data/taegun/prs_revision/data/df_af_match_pcr_final2.csv\",\n",
    "    \"dem\":    \"/Data/taegun/prs_revision/data/df_dem_match_pcr_final2.csv\"\n",
    "}\n",
    "TARGET_NAME = {k: \"Disease_status\" for k in FILES.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39a81c81-5f80-4da4-a008-475aa78fc641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 평가 지표 계산 함수\n",
    "# --------------------------\n",
    "def get_metrics(y_true, y_pred, y_proba):\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    auprc = average_precision_score(y_true, y_proba)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn + 1e-8)\n",
    "    specificity = tn / (tn + fp + 1e-8)\n",
    "\n",
    "    lr_plus = sensitivity / (1 - specificity + 1e-8)\n",
    "    lr_minus = (1 - sensitivity) / (specificity + 1e-8)\n",
    "    dor = lr_plus / (lr_minus + 1e-8)\n",
    "\n",
    "    return auc, auprc, sensitivity, specificity, dor, lr_plus, lr_minus\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LightGBM Nested CV (CPU)\n",
    "# ============================================================\n",
    "def run_lgb_nested_cv_cpu(\n",
    "    X,\n",
    "    y,\n",
    "    sampling_methods=[\"none\", \"undersample\", \"smote\", \"class_weight\"],\n",
    "    sampling_ratios=[0.6, 0.8, 1.0],\n",
    "):\n",
    "\n",
    "    # --------------------------\n",
    "    # Hyperparameter Grid\n",
    "    # --------------------------\n",
    "    lgb_params = {\n",
    "        \"n_estimators\": [100, 300, 500],\n",
    "        \"max_depth\": [3, 5, 10],\n",
    "        \"num_leaves\": [31, 63],\n",
    "        \"learning_rate\": [0.1, 0.3],\n",
    "        \"subsample\": [0.6, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 1.0]\n",
    "    }\n",
    "\n",
    "    outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "    all_inner_logs = []\n",
    "    all_best_models = []\n",
    "    all_outer_tests = []\n",
    "\n",
    "    # ============================================================\n",
    "    # OUTER LOOP\n",
    "    # ============================================================\n",
    "    for outer_fold, (train_idx, test_idx) in enumerate(outer.split(X, y), 1):\n",
    "\n",
    "        X_train_outer = X.iloc[train_idx].copy()\n",
    "        X_test_outer = X.iloc[test_idx].copy()\n",
    "        y_train_outer = y.iloc[train_idx]\n",
    "        y_test_outer = y.iloc[test_idx]\n",
    "\n",
    "        # --------------------------\n",
    "        # Scaling (outer train 기준)\n",
    "        # --------------------------\n",
    "        scaler = StandardScaler()\n",
    "        cols_to_scale = [c for c in X.columns if c not in [\"AGE\", \"SEX\", \"DLVH\"]]\n",
    "\n",
    "        X_train_outer[cols_to_scale] = scaler.fit_transform(X_train_outer[cols_to_scale])\n",
    "        X_test_outer[cols_to_scale] = scaler.transform(X_test_outer[cols_to_scale])\n",
    "\n",
    "        inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "        inner_log = []\n",
    "        avg_auc_list = []\n",
    "\n",
    "        # ============================================================\n",
    "        # INNER LOOP\n",
    "        # ============================================================\n",
    "        for sampling_method in sampling_methods:\n",
    "\n",
    "            ratios = sampling_ratios if sampling_method in [\"undersample\", \"smote\"] else [None]\n",
    "\n",
    "            for ratio in ratios:\n",
    "                for (n_est, max_d, num_leaf, lr, subs, col) in itertools.product(\n",
    "                    lgb_params[\"n_estimators\"],\n",
    "                    lgb_params[\"max_depth\"],\n",
    "                    lgb_params[\"num_leaves\"],\n",
    "                    lgb_params[\"learning_rate\"],\n",
    "                    lgb_params[\"subsample\"],\n",
    "                    lgb_params[\"colsample_bytree\"],\n",
    "                ):\n",
    "\n",
    "                    inner_auc_values = []\n",
    "\n",
    "                    for inner_fold, (tr_idx, val_idx) in enumerate(\n",
    "                        inner.split(X_train_outer, y_train_outer), 1\n",
    "                    ):\n",
    "                        X_tr = X_train_outer.iloc[tr_idx]\n",
    "                        y_tr = y_train_outer.iloc[tr_idx]\n",
    "                        X_val = X_train_outer.iloc[val_idx]\n",
    "                        y_val = y_train_outer.iloc[val_idx]\n",
    "\n",
    "                        # --------------------------\n",
    "                        # Sampling\n",
    "                        # --------------------------\n",
    "                        if sampling_method == \"undersample\":\n",
    "                            sampler = RandomUnderSampler(\n",
    "                                sampling_strategy=ratio, random_state=42\n",
    "                            )\n",
    "                            X_res, y_res = sampler.fit_resample(X_tr, y_tr)\n",
    "\n",
    "                        elif sampling_method == \"smote\":\n",
    "                            sampler = SMOTE(\n",
    "                                sampling_strategy=ratio, random_state=42\n",
    "                            )\n",
    "                            X_res, y_res = sampler.fit_resample(X_tr, y_tr)\n",
    "\n",
    "                        else:  # none, class_weight\n",
    "                            X_res, y_res = X_tr, y_tr\n",
    "\n",
    "                        # --------------------------\n",
    "                        # Model\n",
    "                        # --------------------------\n",
    "                        model = lgb.LGBMClassifier(\n",
    "                            n_estimators=n_est,\n",
    "                            max_depth=max_d,\n",
    "                            num_leaves=num_leaf,\n",
    "                            learning_rate=lr,\n",
    "                            subsample=subs,\n",
    "                            colsample_bytree=col,\n",
    "                            objective=\"binary\",\n",
    "                            class_weight=\"balanced\" if sampling_method == \"class_weight\" else None,\n",
    "                            random_state=42,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=-1\n",
    "                        )\n",
    "\n",
    "                        model.fit(X_res, y_res)\n",
    "\n",
    "                        val_proba = model.predict_proba(X_val)[:, 1]\n",
    "                        val_pred = (val_proba > 0.5).astype(int)\n",
    "                        auc, auprc, sen, spe, dor, lr_p, lr_m = get_metrics(\n",
    "                            y_val, val_pred, val_proba\n",
    "                        )\n",
    "\n",
    "                        inner_auc_values.append(auc)\n",
    "\n",
    "                        inner_log.append({\n",
    "                            \"outer_fold\": outer_fold,\n",
    "                            \"inner_fold\": inner_fold,\n",
    "                            \"sampling_method\": sampling_method,\n",
    "                            \"sampling_ratio\": ratio,\n",
    "                            \"n_estimators\": n_est,\n",
    "                            \"max_depth\": max_d,\n",
    "                            \"num_leaves\": num_leaf,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"subsample\": subs,\n",
    "                            \"colsample_bytree\": col,\n",
    "                            \"AUC\": auc,\n",
    "                            \"AUPRC\" : auprc,\n",
    "                            \"sensitivity\": sen,\n",
    "                            \"specificity\": spe,\n",
    "                            \"DOR\": dor,\n",
    "                            \"LR+\": lr_p,\n",
    "                            \"LR-\": lr_m\n",
    "                        })\n",
    "\n",
    "                    avg_auc_list.append({\n",
    "                        \"outer_fold\": outer_fold,\n",
    "                        \"sampling_method\": sampling_method,\n",
    "                        \"sampling_ratio\": ratio,\n",
    "                        \"n_estimators\": n_est,\n",
    "                        \"max_depth\": max_d,\n",
    "                        \"num_leaves\": num_leaf,\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"subsample\": subs,\n",
    "                        \"colsample_bytree\": col,\n",
    "                        \"mean_AUC\": np.mean(inner_auc_values)\n",
    "                    })\n",
    "\n",
    "        # ============================================================\n",
    "        # BEST MODEL\n",
    "        # ============================================================\n",
    "        best_info = max(avg_auc_list, key=lambda x: x[\"mean_AUC\"])\n",
    "        all_best_models.append(best_info)\n",
    "\n",
    "        # ============================================================\n",
    "        # OUTER TEST\n",
    "        # ============================================================\n",
    "        sm = best_info[\"sampling_method\"]\n",
    "        r = best_info[\"sampling_ratio\"]\n",
    "\n",
    "        if sm == \"undersample\":\n",
    "            sampler = RandomUnderSampler(sampling_strategy=r, random_state=42)\n",
    "            X_res, y_res = sampler.fit_resample(X_train_outer, y_train_outer)\n",
    "        elif sm == \"smote\":\n",
    "            sampler = SMOTE(sampling_strategy=r, random_state=42)\n",
    "            X_res, y_res = sampler.fit_resample(X_train_outer, y_train_outer)\n",
    "        else:\n",
    "            X_res, y_res = X_train_outer, y_train_outer\n",
    "\n",
    "        final_model = lgb.LGBMClassifier(\n",
    "            n_estimators=best_info[\"n_estimators\"],\n",
    "            max_depth=best_info[\"max_depth\"],\n",
    "            num_leaves=best_info[\"num_leaves\"],\n",
    "            learning_rate=best_info[\"learning_rate\"],\n",
    "            subsample=best_info[\"subsample\"],\n",
    "            colsample_bytree=best_info[\"colsample_bytree\"],\n",
    "            class_weight=\"balanced\" if sm == \"class_weight\" else None,\n",
    "            objective=\"binary\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "\n",
    "        final_model.fit(X_res, y_res)\n",
    "\n",
    "        test_proba = final_model.predict_proba(X_test_outer)[:, 1]\n",
    "        test_pred = (test_proba > 0.5).astype(int)\n",
    "        auc, auprc, sen, spe, dor, lr_p, lr_m = get_metrics(\n",
    "            y_test_outer, test_pred, test_proba\n",
    "        )\n",
    "\n",
    "        all_outer_tests.append({\n",
    "            \"outer_fold\": outer_fold,\n",
    "            \"AUC\": auc,\n",
    "            \"AUPRC\": auprc,\n",
    "            \"sensitivity\": sen,\n",
    "            \"specificity\": spe,\n",
    "            \"DOR\": dor,\n",
    "            \"LR+\": lr_p,\n",
    "            \"LR-\": lr_m\n",
    "        })\n",
    "\n",
    "        all_inner_logs.extend(inner_log)\n",
    "\n",
    "    return {\n",
    "        \"inner_log\": pd.DataFrame(all_inner_logs),\n",
    "        \"best_model\": pd.DataFrame(all_best_models),\n",
    "        \"outer_test\": pd.DataFrame(all_outer_tests)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff32dba-9960-48c5-b21b-0499a1672cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "==============================\n",
      "### 질병: chf ###\n",
      "==============================\n",
      ">>> LightGBM Nested CV 시작\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 실행부: run_lgb_nested_cv_cpu 호출 + 결과 저장\n",
    "# ================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 이미 X, y, disease, sampling_methods, sampling_ratios가 정의되어 있다고 가정.\n",
    "    # (정의 안 되어 있으면 여기에 불러오거나 할당하세요.)\n",
    "    #disease_list = [\"dia\", \"chf\", \"chd\", \"stroke\", \"af\", \"dem\"]\n",
    "    disease_list = [\"chf\", \"chd\"]\n",
    "    \n",
    "    for disease in disease_list:\n",
    "\n",
    "        print(f\"\\n\\n\\n==============================\")\n",
    "        print(f\"### 질병: {disease} ###\")\n",
    "        print(\"==============================\")\n",
    "        \n",
    "        df = pd.read_csv(FILES[disease])\n",
    "        features = FEATURES[disease]\n",
    "        target = TARGET_NAME[disease]\n",
    "        \n",
    "        df_sub = df[features + [target]].dropna()\n",
    "        \n",
    "        X = df_sub[features]\n",
    "        y = df_sub[target]\n",
    "    \n",
    "        print(\">>> LightGBM Nested CV 시작\")\n",
    "        res = run_lgb_nested_cv_cpu(\n",
    "            X=X,\n",
    "            y=y\n",
    "        )\n",
    "    #        sampling_methods=sampling_methods,     # 기본값 사용하려면 생략 가능\n",
    "    #        sampling_ratios=sampling_ratios        # 기본값 사용하려면 생략 가능\n",
    "    \n",
    "        # 반환형: {\"inner_log\": df, \"best_model\": df, \"outer_test\": df}\n",
    "        df_inner = res[\"inner_log\"]\n",
    "        df_best = res[\"best_model\"]\n",
    "        df_outer = res[\"outer_test\"]\n",
    "    \n",
    "        # 저장 경로 (너가 쓰던 형식 그대로)\n",
    "        SAVE_DIR = f\"/Data/taegun/prs_revision/nested_results_samplings_0117/lgbm/prs_model/{disease}\"\n",
    "        os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "        df_inner.to_csv(f\"{SAVE_DIR}/{disease}_lgb_nested_cv_all_results.csv\", index=False)\n",
    "        df_best.to_csv(f\"{SAVE_DIR}/{disease}_lgb_nested_cv_best_per_fold.csv\", index=False)\n",
    "        df_outer.to_csv(f\"{SAVE_DIR}/{disease}_lgb_nested_cv_outer_test_results.csv\", index=False)\n",
    "    \n",
    "        print(\">>> 저장 완료:\")\n",
    "        print(f\" - {SAVE_DIR}/{disease}_lgb_nested_cv_all_results.csv\")\n",
    "        print(f\" - {SAVE_DIR}/{disease}_lgb_nested_cv_best_per_fold.csv\")\n",
    "        print(f\" - {SAVE_DIR}/{disease}_lgb_nested_cv_outer_test_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af08547-ade7-47a9-9f5a-3ea89f6aa1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prsml)",
   "language": "python",
   "name": "prsml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
